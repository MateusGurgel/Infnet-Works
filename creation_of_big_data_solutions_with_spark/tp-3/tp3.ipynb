{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 游늭 Parte 1 An치lise Est치tica (Spark SQL + PostgreSQL + S3)",
   "id": "91d450a849b9ed44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1 - Realize a leitura da tabela apostas do PostgreSQL e transforme a coluna timestamp corretamente.",
   "id": "c66e85f2c8f804d4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-03T01:14:52.538221Z",
     "start_time": "2025-06-03T01:14:51.211428Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura PostgreSQL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.27,org.postgresql:postgresql:42.2.27,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/betalert\"\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table = \"apostas\"\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# convertendo a data timestamp de string para timestamp\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)\n",
    "spark.stop()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 22:14:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aposta_id: string (nullable = true)\n",
      " |-- apostador_id: string (nullable = true)\n",
      " |-- jogo_id: string (nullable = true)\n",
      " |-- valor: decimal(38,18) (nullable = true)\n",
      " |-- odd: decimal(38,18) (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- resultado: string (nullable = true)\n",
      "\n",
      "+---------+------------+-------+--------------------+--------------------+-------------------+---------+\n",
      "|aposta_id|apostador_id|jogo_id|               valor|                 odd|          timestamp|resultado|\n",
      "+---------+------------+-------+--------------------+--------------------+-------------------+---------+\n",
      "| b61f4f08|         u79| jogo31|1813.570000000000...|4.490000000000000000|2025-01-05 15:42:00|   perdeu|\n",
      "| 88c44f52|         u23| jogo33|830.6000000000000...|2.870000000000000000|2025-01-05 09:54:00| pendente|\n",
      "| 4c32051c|         u72| jogo72|1829.920000000000...|2.090000000000000000|2025-01-03 23:34:00| pendente|\n",
      "| 1baa492c|         u70| jogo67|1614.200000000000...|4.210000000000000000|2025-01-04 16:50:00|   perdeu|\n",
      "| 2820b7bb|         u40| jogo50|989.7300000000000...|2.980000000000000000|2025-01-02 20:37:00|   perdeu|\n",
      "| 9f953950|         u10| jogo66|1786.550000000000...|3.300000000000000000|2025-01-04 13:58:00|   perdeu|\n",
      "| 1549ba26|         u95| jogo42|683.4800000000000...|3.220000000000000000|2025-01-08 01:18:00|   perdeu|\n",
      "| 029f146f|         u71|  jogo2|1239.370000000000...|4.330000000000000000|2025-01-06 06:19:00|   ganhou|\n",
      "| 4b389fb4|         u51| jogo52|1488.510000000000...|2.060000000000000000|2025-01-02 00:53:00| pendente|\n",
      "| 4171ddaf|         u46| jogo52|401.4100000000000...|4.620000000000000000|2025-01-06 04:19:00| pendente|\n",
      "+---------+------------+-------+--------------------+--------------------+-------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2 - Realize a leitura da tabela transacoes_financeiras e normalize o nome da coluna de valor.",
   "id": "e13efc12988e4c0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:13:12.256268Z",
     "start_time": "2025-06-03T01:13:10.871682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura PostgreSQL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.27\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/betalert\"\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table = \"transacoes_financeiras\"\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# convertendo a coluna para manter s칩 dois valores decimais, mas sem arredondamento para manter o valor real\n",
    "df = df.withColumn(\"valor\", col(\"valor\").cast(DecimalType(10, 2)))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)\n",
    "spark.stop()\n"
   ],
   "id": "b8efd671d2627221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- apostador_id: string (nullable = true)\n",
      " |-- valor: decimal(10,2) (nullable = true)\n",
      " |-- tipo: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      "\n",
      "+---+------------+--------+--------+-------------------+\n",
      "| id|apostador_id|   valor|    tipo|               data|\n",
      "+---+------------+--------+--------+-------------------+\n",
      "|  1|         u69|14890.81|deposito|2025-01-01 10:00:00|\n",
      "|  2|         u94| 5616.48|deposito|2025-01-01 10:01:00|\n",
      "|  3|         u95|16376.42|   saque|2025-01-01 10:02:00|\n",
      "|  4|         u11|15335.80|deposito|2025-01-01 10:03:00|\n",
      "|  5|         u88|19559.30|   saque|2025-01-01 10:04:00|\n",
      "|  6|         u32| 4797.95|deposito|2025-01-01 10:05:00|\n",
      "|  7|         u14| 1494.86|   saque|2025-01-01 10:06:00|\n",
      "|  8|         u23|15609.39|deposito|2025-01-01 10:07:00|\n",
      "|  9|          u8|14916.72|deposito|2025-01-01 10:08:00|\n",
      "| 10|         u92|12104.17|   saque|2025-01-01 10:09:00|\n",
      "+---+------------+--------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3 - Fa칞a o join entre apostas e o arquivo apostadores.csv do S3 para incluir o pa칤s e dados extras.",
   "id": "9aeeb65f3a511be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:13:15.088894Z",
     "start_time": "2025-06-03T01:13:13.525748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura PostgreSQL\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/betalert\"\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Em tese, o join faz uma combina칞칚o de cada saque por aposta, ent칚o seria necess치rio s칩 comparar\n",
    "# as diferen칞as entre saque e dep칩sito.\n",
    "\n",
    "transactions = spark.read.jdbc(url=url, table=\"transacoes_financeiras\", properties=properties)\n",
    "bets = spark.read.jdbc(url=url, table=\"apostas\", properties=properties)\n",
    "\n",
    "apostadores = spark.read.csv(\"s3a://betalogs/apostadores.csv\", header=True, inferSchema=True)\n",
    "\n",
    "bets = bets.withColumnRenamed(\"valor\", \"bet_valor\")\n",
    "transactions = transactions.withColumnRenamed(\"valor\", \"transaction_valor\")\n",
    "\n",
    "bets = bets.withColumnRenamed(\"pais\", \"pais_bet\")\n",
    "transactions = transactions.withColumnRenamed(\"pais\", \"pais_transacao\")\n",
    "\n",
    "\n",
    "bets_transactions = transactions.join(\n",
    "    bets, transactions.apostador_id == bets.apostador_id, \"inner\"\n",
    ").drop(bets[\"apostador_id\"])\n",
    "\n",
    "resultado_final = bets_transactions.join(\n",
    "    apostadores, bets_transactions.apostador_id == apostadores.id, \"inner\"\n",
    ").drop(apostadores[\"id\"])\n",
    "\n",
    "resultado_final.show(10)\n"
   ],
   "id": "fda90dd039e980f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 22:13:14 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://betalogs/apostadores.csv.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o136.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m transactions \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjdbc(url\u001B[38;5;241m=\u001B[39murl, table\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransacoes_financeiras\u001B[39m\u001B[38;5;124m\"\u001B[39m, properties\u001B[38;5;241m=\u001B[39mproperties)\n\u001B[1;32m     25\u001B[0m bets \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjdbc(url\u001B[38;5;241m=\u001B[39murl, table\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapostas\u001B[39m\u001B[38;5;124m\"\u001B[39m, properties\u001B[38;5;241m=\u001B[39mproperties)\n\u001B[0;32m---> 27\u001B[0m apostadores \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ms3a://betalogs/apostadores.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minferSchema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m bets \u001B[38;5;241m=\u001B[39m bets\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbet_valor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     30\u001B[0m transactions \u001B[38;5;241m=\u001B[39m transactions\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransaction_valor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o136.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 游댌 Parte 2 Detec칞칚o de Padr칫es",
   "id": "5241fc55c8ff6206"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observando se uma aposta 칠 Flash por meio da subtra칞칚o de duas time stamps em unix (segundos desde 1970)",
   "id": "8848aa37808b7880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import unix_timestamp, abs\n",
    "\n",
    "deposits = resultado_final.where(resultado_final.tipo == \"deposito\")\n",
    "\n",
    "# depositos onde a diferen칞a entre data e timestamp 칠 menor que 10 segundos\n",
    "flash_deposits = deposits.where(\n",
    "    abs(unix_timestamp(\"data\") - unix_timestamp(\"timestamp\")) < 10\n",
    ")\n",
    "\n",
    "flash_deposits.show(20)"
   ],
   "id": "9c0a5db45f833b98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exiba apostas-rel칙mpago com valor acima de R$500.",
   "id": "86454504119c02d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gt_500_flash_deposits = flash_deposits.where(flash_deposits.bet_valor > 500)\n",
    "gt_500_flash_deposits.show(20)"
   ],
   "id": "4983c861f9bb26a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exiba apostas-rel칙mpago com valor acima de R$10.000.",
   "id": "e28085424fdc8a7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gt_10000_flash_deposits = flash_deposits.where(flash_deposits.bet_valor > 10000)\n",
    "gt_10000_flash_deposits.show()"
   ],
   "id": "4039d753b0aec6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Detecte jogadores que realizaram 10 ou mais apostas em um mesmo jogo.",
   "id": "ad6b7a202c5bf19a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "heavy_bettors = bets.groupBy(\"apostador_id\", \"jogo_id\") \\\n",
    "    .agg(count(\"*\").alias(\"num_apostas\")) \\\n",
    "    .where(col(\"num_apostas\") >= 10)\n",
    "\n",
    "heavy_bettors.show()"
   ],
   "id": "1568ee90c91dbeaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exiba o total e a m칠dia de valores apostados por pa칤s.",
   "id": "bf61a10dc5c99074"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "bets_by_country = resultado_final.groupBy(\"pais\").agg(\n",
    "    sum(\"bet_valor\").alias(\"total_apostado\"),\n",
    "    avg(\"bet_valor\").alias(\"media_apostada\")\n",
    ")\n",
    "\n",
    "bets_by_country.show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "746857c6e2accadd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 游니 Parte 3 Streaming em Tempo Real (Kafka + Spark Structured Streaming)\n",
   "id": "88b6ff9da500de2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Modifiquei pesadamente o docker compose para conseguir usar o jupyter notebook no projeto. Extrai os scripts em python da imagem, e removi a network do docker compose, mapeando todas as portas na minha m치quina, j치 que n칚o foi poss칤vel consumir o kafka na rede virtual do docker",
   "id": "da2ea47387145195"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, when\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, FloatType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.27,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Kafka\n",
    "df = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,localhost:9093\") \\\n",
    "    .option('subscribe', 'stream_apostas') \\\n",
    "    .load()\n",
    "\n",
    "# Leitura do CSV com apostadores\n",
    "apostadores = spark.read.csv(\"s3a://betalogs/apostadores.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Schema da mensagem Kafka\n",
    "schema = (StructType()\n",
    "          .add(\"aposta_id\", StringType())\n",
    "          .add(\"apostador_id\", StringType())\n",
    "          .add(\"jogo_id\", StringType())\n",
    "          .add(\"valor\", FloatType())\n",
    "          .add(\"odd\", FloatType())\n",
    "          .add(\"timestamp\", TimestampType())\n",
    "          )\n",
    "\n",
    "# Parse do JSON + transforma칞칚o\n",
    "df_valores = df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"dados\")) \\\n",
    "    .select(\"dados.*\")\n",
    "\n",
    "# Join com apostadores\n",
    "df_valores = df_valores.join(apostadores, apostadores.id == df_valores.apostador_id, \"inner\")\n",
    "\n",
    "# 游뚿 Coluna 'suspeita' para apostas acima de R$12.000 e odd acima de 15\n",
    "df_valores = df_valores.withColumn(\n",
    "    \"suspeita\",\n",
    "    when((col(\"valor\") > 12000) & (col(\"odd\") > 15), True).otherwise(False)\n",
    ")\n",
    "\n",
    "# Sa칤da no console\n",
    "query = df_valores.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "id": "4f6d5b5876bd2d95",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
