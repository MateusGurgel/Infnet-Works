{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìÇ Parte 1 An√°lise Est√°tica (Spark SQL + PostgreSQL + S3)",
   "id": "91d450a849b9ed44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1 - Realize a leitura da tabela apostas do PostgreSQL e transforme a coluna timestamp corretamente.",
   "id": "c66e85f2c8f804d4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-03T01:25:51.673414Z",
     "start_time": "2025-06-03T01:25:25.302843Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura PostgreSQL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.27,org.postgresql:postgresql:42.2.27,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/betalert\"\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table = \"apostas\"\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# convertendo a data timestamp de string para timestamp\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)\n",
    "spark.stop()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/mateus/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mateus/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mateus/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-81ac969b-70ab-4991-b8d8-87c512a61a2b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.27 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1838ms :: artifacts dl 68ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.27 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-81ac969b-70ab-4991-b8d8-87c512a61a2b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/36ms)\n",
      "25/06/02 22:25:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aposta_id: string (nullable = true)\n",
      " |-- apostador_id: string (nullable = true)\n",
      " |-- jogo_id: string (nullable = true)\n",
      " |-- valor: decimal(38,18) (nullable = true)\n",
      " |-- odd: decimal(38,18) (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- resultado: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------+--------------------+--------------------+-------------------+---------+\n",
      "|aposta_id|apostador_id|jogo_id|               valor|                 odd|          timestamp|resultado|\n",
      "+---------+------------+-------+--------------------+--------------------+-------------------+---------+\n",
      "| b61f4f08|         u79| jogo31|1813.570000000000...|4.490000000000000000|2025-01-05 15:42:00|   perdeu|\n",
      "| 88c44f52|         u23| jogo33|830.6000000000000...|2.870000000000000000|2025-01-05 09:54:00| pendente|\n",
      "| 4c32051c|         u72| jogo72|1829.920000000000...|2.090000000000000000|2025-01-03 23:34:00| pendente|\n",
      "| 1baa492c|         u70| jogo67|1614.200000000000...|4.210000000000000000|2025-01-04 16:50:00|   perdeu|\n",
      "| 2820b7bb|         u40| jogo50|989.7300000000000...|2.980000000000000000|2025-01-02 20:37:00|   perdeu|\n",
      "| 9f953950|         u10| jogo66|1786.550000000000...|3.300000000000000000|2025-01-04 13:58:00|   perdeu|\n",
      "| 1549ba26|         u95| jogo42|683.4800000000000...|3.220000000000000000|2025-01-08 01:18:00|   perdeu|\n",
      "| 029f146f|         u71|  jogo2|1239.370000000000...|4.330000000000000000|2025-01-06 06:19:00|   ganhou|\n",
      "| 4b389fb4|         u51| jogo52|1488.510000000000...|2.060000000000000000|2025-01-02 00:53:00| pendente|\n",
      "| 4171ddaf|         u46| jogo52|401.4100000000000...|4.620000000000000000|2025-01-06 04:19:00| pendente|\n",
      "+---------+------------+-------+--------------------+--------------------+-------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2 - Realize a leitura da tabela transacoes_financeiras e normalize o nome da coluna de valor.",
   "id": "e13efc12988e4c0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:25:57.309277Z",
     "start_time": "2025-06-03T01:25:52.451352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura PostgreSQL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.27\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/betalert\"\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table = \"transacoes_financeiras\"\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# convertendo a coluna para manter s√≥ dois valores decimais, mas sem arredondamento para manter o valor real\n",
    "df = df.withColumn(\"valor\", col(\"valor\").cast(DecimalType(10, 2)))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)\n",
    "spark.stop()\n"
   ],
   "id": "b8efd671d2627221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- apostador_id: string (nullable = true)\n",
      " |-- valor: decimal(10,2) (nullable = true)\n",
      " |-- tipo: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+--------+-------------------+\n",
      "| id|apostador_id|   valor|    tipo|               data|\n",
      "+---+------------+--------+--------+-------------------+\n",
      "|  1|         u69|14890.81|deposito|2025-01-01 10:00:00|\n",
      "|  2|         u94| 5616.48|deposito|2025-01-01 10:01:00|\n",
      "|  3|         u95|16376.42|   saque|2025-01-01 10:02:00|\n",
      "|  4|         u11|15335.80|deposito|2025-01-01 10:03:00|\n",
      "|  5|         u88|19559.30|   saque|2025-01-01 10:04:00|\n",
      "|  6|         u32| 4797.95|deposito|2025-01-01 10:05:00|\n",
      "|  7|         u14| 1494.86|   saque|2025-01-01 10:06:00|\n",
      "|  8|         u23|15609.39|deposito|2025-01-01 10:07:00|\n",
      "|  9|          u8|14916.72|deposito|2025-01-01 10:08:00|\n",
      "| 10|         u92|12104.17|   saque|2025-01-01 10:09:00|\n",
      "+---+------------+--------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3 - Fa√ßa o join entre apostas e o arquivo apostadores.csv do S3 para incluir o pa√≠s e dados extras.",
   "id": "9aeeb65f3a511be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:26:09.842828Z",
     "start_time": "2025-06-03T01:25:59.058270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura PostgreSQL\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/betalert\"\n",
    "properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Em tese, o join faz uma combina√ß√£o de cada saque por aposta, ent√£o seria necess√°rio s√≥ comparar\n",
    "# as diferen√ßas entre saque e dep√≥sito.\n",
    "\n",
    "transactions = spark.read.jdbc(url=url, table=\"transacoes_financeiras\", properties=properties)\n",
    "bets = spark.read.jdbc(url=url, table=\"apostas\", properties=properties)\n",
    "\n",
    "apostadores = spark.read.csv(\"s3a://betalogs/apostadores.csv\", header=True, inferSchema=True)\n",
    "\n",
    "bets = bets.withColumnRenamed(\"valor\", \"bet_valor\")\n",
    "transactions = transactions.withColumnRenamed(\"valor\", \"transaction_valor\")\n",
    "\n",
    "bets = bets.withColumnRenamed(\"pais\", \"pais_bet\")\n",
    "transactions = transactions.withColumnRenamed(\"pais\", \"pais_transacao\")\n",
    "\n",
    "\n",
    "bets_transactions = transactions.join(\n",
    "    bets, transactions.apostador_id == bets.apostador_id, \"inner\"\n",
    ").drop(bets[\"apostador_id\"])\n",
    "\n",
    "resultado_final = bets_transactions.join(\n",
    "    apostadores, bets_transactions.apostador_id == apostadores.id, \"inner\"\n",
    ").drop(apostadores[\"id\"])\n",
    "\n",
    "resultado_final.show(10)\n"
   ],
   "id": "fda90dd039e980f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 22:26:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "| id|apostador_id|   transaction_valor|    tipo|               data|aposta_id|jogo_id|           bet_valor|                 odd|          timestamp|resultado|      nome|               email|pais|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "|411|         u42|12961.78000000000...|deposito|2025-01-01 16:50:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|403|         u42|3849.670000000000...|deposito|2025-01-01 16:42:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|277|         u42|1351.900000000000...|deposito|2025-01-01 14:36:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|237|         u42|5012.580000000000...|deposito|2025-01-01 13:56:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|207|         u42|9855.530000000000...|deposito|2025-01-01 13:26:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|144|         u42|8355.710000000000...|   saque|2025-01-01 12:23:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "| 31|         u42|1052.520000000000...|deposito|2025-01-01 10:30:00| be7645ac|  jogo2|1776.830000000000...|4.700000000000000000|2025-01-03 11:29:00| pendente|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|411|         u42|12961.78000000000...|deposito|2025-01-01 16:50:00| 294036ab| jogo47|61.46000000000000...|2.150000000000000000|2025-01-07 16:19:00|   perdeu|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|403|         u42|3849.670000000000...|deposito|2025-01-01 16:42:00| 294036ab| jogo47|61.46000000000000...|2.150000000000000000|2025-01-07 16:19:00|   perdeu|Jogador 42|jogador42@exemplo...|  ES|\n",
      "|277|         u42|1351.900000000000...|deposito|2025-01-01 14:36:00| 294036ab| jogo47|61.46000000000000...|2.150000000000000000|2025-01-07 16:19:00|   perdeu|Jogador 42|jogador42@exemplo...|  ES|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üîç Parte 2 Detec√ß√£o de Padr√µes",
   "id": "5241fc55c8ff6206"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observando se uma aposta √© Flash por meio da subtra√ß√£o de duas time stamps em unix (segundos desde 1970)",
   "id": "8848aa37808b7880"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:26:12.344866Z",
     "start_time": "2025-06-03T01:26:10.825948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import unix_timestamp, abs\n",
    "\n",
    "deposits = resultado_final.where(resultado_final.tipo == \"deposito\")\n",
    "\n",
    "# depositos onde a diferen√ßa entre data e timestamp √© menor que 10 segundos\n",
    "flash_deposits = deposits.where(\n",
    "    abs(unix_timestamp(\"data\") - unix_timestamp(\"timestamp\")) < 10\n",
    ")\n",
    "\n",
    "flash_deposits.show(20)"
   ],
   "id": "9c0a5db45f833b98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "| id|apostador_id|   transaction_valor|    tipo|               data|aposta_id|jogo_id|           bet_valor|                 odd|          timestamp|resultado|      nome|               email|pais|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "|505|         u73|15000.00000000000...|deposito|2025-01-11 20:04:00| db280beb|  jogo3|15000.00000000000...|24.54000000000000...|2025-01-11 20:04:07|   ganhou|Jogador 73|jogador73@exemplo...|  ES|\n",
      "|510|         u66|15000.00000000000...|deposito|2025-01-11 20:09:00| 0c7defda|  jogo6|15000.00000000000...|12.23000000000000...|2025-01-11 20:09:09|   ganhou|Jogador 66|jogador66@exemplo...|  AR|\n",
      "|173|         u19|8595.740000000000...|deposito|2025-01-01 12:52:00| df00489c| jogo95|574.7100000000000...|2.600000000000000000|2025-01-01 12:52:00|   ganhou|Jogador 19|jogador19@exemplo...|  BR|\n",
      "|508|         u99|15000.00000000000...|deposito|2025-01-11 20:07:00| dbf3315b| jogo16|15000.00000000000...|11.54000000000000...|2025-01-11 20:07:03|   ganhou|Jogador 99|jogador99@exemplo...|  AR|\n",
      "|504|         u58|15000.00000000000...|deposito|2025-01-11 20:03:00| 34c39892| jogo60|15000.00000000000...|11.54000000000000...|2025-01-11 20:03:09|   ganhou|Jogador 58|jogador58@exemplo...|  BR|\n",
      "|509|         u11|15000.00000000000...|deposito|2025-01-11 20:08:00| 727a0ad2| jogo39|15000.00000000000...|21.33000000000000...|2025-01-11 20:08:07|   ganhou|Jogador 11|jogador11@exemplo...|  BR|\n",
      "|502|         u33|15000.00000000000...|deposito|2025-01-11 20:01:00| d6002cae| jogo95|15000.00000000000...|14.60000000000000...|2025-01-11 20:01:01|   ganhou|Jogador 33|jogador33@exemplo...|  ES|\n",
      "|503|         u41|15000.00000000000...|deposito|2025-01-11 20:02:00| 066f9055| jogo71|15000.00000000000...|16.46000000000000...|2025-01-11 20:02:09|   ganhou|Jogador 41|jogador41@exemplo...|  ES|\n",
      "|501|         u17|15000.00000000000...|deposito|2025-01-11 20:00:00| 5b0e994f|  jogo8|15000.00000000000...|15.32000000000000...|2025-01-11 20:00:02|   ganhou|Jogador 17|jogador17@exemplo...|  FR|\n",
      "|506|         u88|15000.00000000000...|deposito|2025-01-11 20:05:00| 59c22669| jogo78|15000.00000000000...|17.90000000000000...|2025-01-11 20:05:08|   ganhou|Jogador 88|jogador88@exemplo...|  ES|\n",
      "|507|         u95|15000.00000000000...|deposito|2025-01-11 20:06:00| df1619c3| jogo25|15000.00000000000...|13.44000000000000...|2025-01-11 20:06:09|   ganhou|Jogador 95|jogador95@exemplo...|  ES|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exiba apostas-rel√¢mpago com valor acima de R$500.",
   "id": "86454504119c02d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:26:13.782015Z",
     "start_time": "2025-06-03T01:26:12.565918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gt_500_flash_deposits = flash_deposits.where(flash_deposits.bet_valor > 500)\n",
    "gt_500_flash_deposits.show(20)"
   ],
   "id": "4983c861f9bb26a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "| id|apostador_id|   transaction_valor|    tipo|               data|aposta_id|jogo_id|           bet_valor|                 odd|          timestamp|resultado|      nome|               email|pais|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "|505|         u73|15000.00000000000...|deposito|2025-01-11 20:04:00| db280beb|  jogo3|15000.00000000000...|24.54000000000000...|2025-01-11 20:04:07|   ganhou|Jogador 73|jogador73@exemplo...|  ES|\n",
      "|510|         u66|15000.00000000000...|deposito|2025-01-11 20:09:00| 0c7defda|  jogo6|15000.00000000000...|12.23000000000000...|2025-01-11 20:09:09|   ganhou|Jogador 66|jogador66@exemplo...|  AR|\n",
      "|173|         u19|8595.740000000000...|deposito|2025-01-01 12:52:00| df00489c| jogo95|574.7100000000000...|2.600000000000000000|2025-01-01 12:52:00|   ganhou|Jogador 19|jogador19@exemplo...|  BR|\n",
      "|508|         u99|15000.00000000000...|deposito|2025-01-11 20:07:00| dbf3315b| jogo16|15000.00000000000...|11.54000000000000...|2025-01-11 20:07:03|   ganhou|Jogador 99|jogador99@exemplo...|  AR|\n",
      "|504|         u58|15000.00000000000...|deposito|2025-01-11 20:03:00| 34c39892| jogo60|15000.00000000000...|11.54000000000000...|2025-01-11 20:03:09|   ganhou|Jogador 58|jogador58@exemplo...|  BR|\n",
      "|509|         u11|15000.00000000000...|deposito|2025-01-11 20:08:00| 727a0ad2| jogo39|15000.00000000000...|21.33000000000000...|2025-01-11 20:08:07|   ganhou|Jogador 11|jogador11@exemplo...|  BR|\n",
      "|502|         u33|15000.00000000000...|deposito|2025-01-11 20:01:00| d6002cae| jogo95|15000.00000000000...|14.60000000000000...|2025-01-11 20:01:01|   ganhou|Jogador 33|jogador33@exemplo...|  ES|\n",
      "|503|         u41|15000.00000000000...|deposito|2025-01-11 20:02:00| 066f9055| jogo71|15000.00000000000...|16.46000000000000...|2025-01-11 20:02:09|   ganhou|Jogador 41|jogador41@exemplo...|  ES|\n",
      "|501|         u17|15000.00000000000...|deposito|2025-01-11 20:00:00| 5b0e994f|  jogo8|15000.00000000000...|15.32000000000000...|2025-01-11 20:00:02|   ganhou|Jogador 17|jogador17@exemplo...|  FR|\n",
      "|506|         u88|15000.00000000000...|deposito|2025-01-11 20:05:00| 59c22669| jogo78|15000.00000000000...|17.90000000000000...|2025-01-11 20:05:08|   ganhou|Jogador 88|jogador88@exemplo...|  ES|\n",
      "|507|         u95|15000.00000000000...|deposito|2025-01-11 20:06:00| df1619c3| jogo25|15000.00000000000...|13.44000000000000...|2025-01-11 20:06:09|   ganhou|Jogador 95|jogador95@exemplo...|  ES|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exiba apostas-rel√¢mpago com valor acima de R$10.000.",
   "id": "e28085424fdc8a7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:26:14.931251Z",
     "start_time": "2025-06-03T01:26:14.030836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gt_10000_flash_deposits = flash_deposits.where(flash_deposits.bet_valor > 10000)\n",
    "gt_10000_flash_deposits.show()"
   ],
   "id": "4039d753b0aec6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "| id|apostador_id|   transaction_valor|    tipo|               data|aposta_id|jogo_id|           bet_valor|                 odd|          timestamp|resultado|      nome|               email|pais|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "|505|         u73|15000.00000000000...|deposito|2025-01-11 20:04:00| db280beb|  jogo3|15000.00000000000...|24.54000000000000...|2025-01-11 20:04:07|   ganhou|Jogador 73|jogador73@exemplo...|  ES|\n",
      "|510|         u66|15000.00000000000...|deposito|2025-01-11 20:09:00| 0c7defda|  jogo6|15000.00000000000...|12.23000000000000...|2025-01-11 20:09:09|   ganhou|Jogador 66|jogador66@exemplo...|  AR|\n",
      "|508|         u99|15000.00000000000...|deposito|2025-01-11 20:07:00| dbf3315b| jogo16|15000.00000000000...|11.54000000000000...|2025-01-11 20:07:03|   ganhou|Jogador 99|jogador99@exemplo...|  AR|\n",
      "|504|         u58|15000.00000000000...|deposito|2025-01-11 20:03:00| 34c39892| jogo60|15000.00000000000...|11.54000000000000...|2025-01-11 20:03:09|   ganhou|Jogador 58|jogador58@exemplo...|  BR|\n",
      "|509|         u11|15000.00000000000...|deposito|2025-01-11 20:08:00| 727a0ad2| jogo39|15000.00000000000...|21.33000000000000...|2025-01-11 20:08:07|   ganhou|Jogador 11|jogador11@exemplo...|  BR|\n",
      "|502|         u33|15000.00000000000...|deposito|2025-01-11 20:01:00| d6002cae| jogo95|15000.00000000000...|14.60000000000000...|2025-01-11 20:01:01|   ganhou|Jogador 33|jogador33@exemplo...|  ES|\n",
      "|503|         u41|15000.00000000000...|deposito|2025-01-11 20:02:00| 066f9055| jogo71|15000.00000000000...|16.46000000000000...|2025-01-11 20:02:09|   ganhou|Jogador 41|jogador41@exemplo...|  ES|\n",
      "|501|         u17|15000.00000000000...|deposito|2025-01-11 20:00:00| 5b0e994f|  jogo8|15000.00000000000...|15.32000000000000...|2025-01-11 20:00:02|   ganhou|Jogador 17|jogador17@exemplo...|  FR|\n",
      "|506|         u88|15000.00000000000...|deposito|2025-01-11 20:05:00| 59c22669| jogo78|15000.00000000000...|17.90000000000000...|2025-01-11 20:05:08|   ganhou|Jogador 88|jogador88@exemplo...|  ES|\n",
      "|507|         u95|15000.00000000000...|deposito|2025-01-11 20:06:00| df1619c3| jogo25|15000.00000000000...|13.44000000000000...|2025-01-11 20:06:09|   ganhou|Jogador 95|jogador95@exemplo...|  ES|\n",
      "+---+------------+--------------------+--------+-------------------+---------+-------+--------------------+--------------------+-------------------+---------+----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Detecte jogadores que realizaram 10 ou mais apostas em um mesmo jogo.",
   "id": "ad6b7a202c5bf19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:26:16.844396Z",
     "start_time": "2025-06-03T01:26:15.170432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "heavy_bettors = bets.groupBy(\"apostador_id\", \"jogo_id\") \\\n",
    "    .agg(count(\"*\").alias(\"num_apostas\")) \\\n",
    "    .where(col(\"num_apostas\") >= 10)\n",
    "\n",
    "heavy_bettors.show()"
   ],
   "id": "1568ee90c91dbeaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+\n",
      "|apostador_id|jogo_id|num_apostas|\n",
      "+------------+-------+-----------+\n",
      "|         u88| jogo77|         13|\n",
      "+------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exiba o total e a m√©dia de valores apostados por pa√≠s.",
   "id": "bf61a10dc5c99074"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:26:20.127310Z",
     "start_time": "2025-06-03T01:26:17.079551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "bets_by_country = resultado_final.groupBy(\"pais\").agg(\n",
    "    sum(\"bet_valor\").alias(\"total_apostado\"),\n",
    "    avg(\"bet_valor\").alias(\"media_apostada\")\n",
    ")\n",
    "\n",
    "bets_by_country.show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "746857c6e2accadd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|pais|      total_apostado|      media_apostada|\n",
      "+----+--------------------+--------------------+\n",
      "|  PT|3343853.040000000...|1029.511403940886...|\n",
      "|  BR|5107855.500000000...|1078.744561774023...|\n",
      "|  ES|7238634.930000000...|1082.169970100164...|\n",
      "|  FR|6773318.370000000...|1025.017913135593...|\n",
      "|  AR|4756412.960000000...|1100.512022211938...|\n",
      "+----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üì° Parte 3 Streaming em Tempo Real (Kafka + Spark Structured Streaming)\n",
   "id": "88b6ff9da500de2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Modifiquei pesadamente o docker compose para conseguir usar o jupyter notebook no projeto. Extrai os scripts em python da imagem, e removi a network do docker compose, mapeando todas as portas na minha m√°quina, j√° que n√£o foi poss√≠vel consumir o kafka na rede virtual do docker",
   "id": "da2ea47387145195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T01:51:09.366814Z",
     "start_time": "2025-06-03T01:51:08.317459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, when, avg, count\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, FloatType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.27,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Removendo o log level para limpar o streaming\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,localhost:9093\") \\\n",
    "    .option('subscribe', 'stream_apostas') \\\n",
    "    .load()\n",
    "\n",
    "# Leitura do CSV com apostadores na S3\n",
    "bettors_df = spark.read.csv(\"s3a://betalogs/apostadores.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Schema da mensagem Kafka\n",
    "message_schema = StructType() \\\n",
    "    .add(\"bet_id\", StringType()) \\\n",
    "    .add(\"bettor_id\", StringType()) \\\n",
    "    .add(\"game_id\", StringType()) \\\n",
    "    .add(\"amount\", FloatType()) \\\n",
    "    .add(\"odd\", FloatType()) \\\n",
    "    .add(\"timestamp\", TimestampType())\n",
    "\n",
    "# Parse do JSON + transforma√ß√£o\n",
    "parsed_stream = kafka_stream.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), message_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Join com apostadores\n",
    "joined_df = parsed_stream.join(bettors_df, bettors_df.id == parsed_stream.bettor_id, \"inner\")\n",
    "\n",
    "joined_df.printSchema()\n",
    "\n",
    "# Coluna de suspeita\n",
    "enriched_df = joined_df.withColumn(\n",
    "    \"suspicious\",\n",
    "    when((col(\"amount\") > 12000) & (col(\"odd\") > 15), True).otherwise(False)\n",
    ")\n",
    "\n",
    "# M√©dia de valor por pa√≠s.\n",
    "avg_amount_by_country = enriched_df.groupBy(\"pais\").agg(avg(\"amount\").alias(\"media_valor\"))\n",
    "\n",
    "# Contagem de apostas suspeitas por pa√≠s.\n",
    "suspicious_count_by_country = enriched_df.groupBy(\"pais\") \\\n",
    "    .agg(count(when(col(\"suspicious\") == True, \"suspicious\")).alias(\"suspicious_bets\"))\n",
    "\n",
    "# Volume total de apostas.\n",
    "total_bets = enriched_df.groupBy().agg(count(\"*\").alias(\"total_bets\"))\n",
    "\n",
    "# Contagem de apostas suspeitas por apostador.\n",
    "\n",
    "query = avg_amount_by_country.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ],
   "id": "4f6d5b5876bd2d95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bet_id: string (nullable = true)\n",
      " |-- bettor_id: string (nullable = true)\n",
      " |-- game_id: string (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- odd: float (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 22:51:09 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f9fba928-588a-4525-883b-aa5e19b9a74a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/06/02 22:51:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [pais#3417], [pais#3417, avg(amount#3429) AS media_valor#3483]\n+- Project [bet_id#3426, bettor_id#3427, game_id#3428, amount#3429, odd#3430, timestamp#3431, id#3414, nome#3415, email#3416, pais#3417, CASE WHEN ((amount#3429 > cast(12000 as float)) AND (odd#3430 > cast(15 as float))) THEN true ELSE false END AS suspicious#3459]\n   +- Join Inner, (id#3414 = bettor_id#3427)\n      :- Project [data#3424.bet_id AS bet_id#3426, data#3424.bettor_id AS bettor_id#3427, data#3424.game_id AS game_id#3428, data#3424.amount AS amount#3429, data#3424.odd AS odd#3430, data#3424.timestamp AS timestamp#3431]\n      :  +- Project [from_json(StructField(bet_id,StringType,true), StructField(bettor_id,StringType,true), StructField(game_id,StringType,true), StructField(amount,FloatType,true), StructField(odd,FloatType,true), StructField(timestamp,TimestampType,true), json_str#3422, Some(America/Belem)) AS data#3424]\n      :     +- Project [cast(value#3384 as string) AS json_str#3422]\n      :        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@17627fd3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@63397291, [kafka.bootstrap.servers=localhost:9092,localhost:9093, subscribe=stream_apostas], [key#3383, value#3384, topic#3385, partition#3386, offset#3387L, timestamp#3388, timestampType#3389], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3bebac7f,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092,localhost:9093, subscribe -> stream_apostas),None), kafka, [key#3376, value#3377, topic#3378, partition#3379, offset#3380L, timestamp#3381, timestampType#3382]\n      +- Relation [id#3414,nome#3415,email#3416,pais#3417] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 69\u001B[0m\n\u001B[1;32m     61\u001B[0m total_bets \u001B[38;5;241m=\u001B[39m enriched_df\u001B[38;5;241m.\u001B[39mgroupBy()\u001B[38;5;241m.\u001B[39magg(count(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_bets\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Contagem de apostas suspeitas por apostador.\u001B[39;00m\n\u001B[1;32m     65\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[43mavg_amount_by_country\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriteStream\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconsole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrigger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessingTime\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m1 minute\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtruncate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m---> 69\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/streaming/readwriter.py:1527\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m   1525\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqueryName(queryName)\n\u001B[1;32m   1526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1528\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mstart(path))\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [pais#3417], [pais#3417, avg(amount#3429) AS media_valor#3483]\n+- Project [bet_id#3426, bettor_id#3427, game_id#3428, amount#3429, odd#3430, timestamp#3431, id#3414, nome#3415, email#3416, pais#3417, CASE WHEN ((amount#3429 > cast(12000 as float)) AND (odd#3430 > cast(15 as float))) THEN true ELSE false END AS suspicious#3459]\n   +- Join Inner, (id#3414 = bettor_id#3427)\n      :- Project [data#3424.bet_id AS bet_id#3426, data#3424.bettor_id AS bettor_id#3427, data#3424.game_id AS game_id#3428, data#3424.amount AS amount#3429, data#3424.odd AS odd#3430, data#3424.timestamp AS timestamp#3431]\n      :  +- Project [from_json(StructField(bet_id,StringType,true), StructField(bettor_id,StringType,true), StructField(game_id,StringType,true), StructField(amount,FloatType,true), StructField(odd,FloatType,true), StructField(timestamp,TimestampType,true), json_str#3422, Some(America/Belem)) AS data#3424]\n      :     +- Project [cast(value#3384 as string) AS json_str#3422]\n      :        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@17627fd3, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@63397291, [kafka.bootstrap.servers=localhost:9092,localhost:9093, subscribe=stream_apostas], [key#3383, value#3384, topic#3385, partition#3386, offset#3387L, timestamp#3388, timestampType#3389], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3bebac7f,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092,localhost:9093, subscribe -> stream_apostas),None), kafka, [key#3376, value#3377, topic#3378, partition#3379, offset#3380L, timestamp#3381, timestampType#3382]\n      +- Relation [id#3414,nome#3415,email#3416,pais#3417] csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "reposit√≥rio no github: https://github.com/MateusGurgel/Infnet-Works/tree/main/creation_of_big_data_solutions_with_spark/tp-3",
   "id": "f29bafd22a973e61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
